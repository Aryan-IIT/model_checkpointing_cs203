{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aryan/Desktop/Academics /Semester 4/AI Software tools and techniques/model_checkpointing_cs203/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset Preparation (10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            sentence  label\n",
      "0  a stirring , funny and finally transporting re...      1\n",
      "1  apparently reassembled from the cutting room f...      0\n",
      "2  they presume their audience wo n't sit still f...      0\n",
      "3  this is a visually stunning rumination on love...      1\n",
      "4  jonathan parker 's bartleby should have been t...      1\n",
      "\n",
      "\n",
      "Number of training samples: 5536\n",
      "Number of validation samples: 1384\n",
      "Number of testing samples: 1821\n"
     ]
    }
   ],
   "source": [
    "# Define correct column names\n",
    "column_names = [\"sentence\", \"label\"]\n",
    "\n",
    "def load_dataset1(train_url, test_url):\n",
    "    train_df = pd.read_csv(train_url, sep='\\t', names=column_names, header=None)\n",
    "    test_df = pd.read_csv(test_url, sep='\\t', names=column_names, header=None)\n",
    "    return train_df, test_df\n",
    "\n",
    "# URLs for SST2 dataset\n",
    "train_url = \"https://raw.githubusercontent.com/clairett/pytorch-sentiment-classification/master/data/SST2/train.tsv\"\n",
    "test_url = \"https://raw.githubusercontent.com/clairett/pytorch-sentiment-classification/master/data/SST2/test.tsv\"\n",
    "\n",
    "# Load datasets\n",
    "train_df, test_df = load_dataset1(train_url, test_url)\n",
    "\n",
    "# Display first few rows to confirm correct loading\n",
    "print(train_df.head())\n",
    "\n",
    "\n",
    "# Use the 'sentence' column since SST2 uses it instead of 'text'\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_df['sentence'], train_df['label'], test_size=0.2, random_state=42\n",
    ")\n",
    "print(\"\\n\")\n",
    "print(f\"Number of training samples: {len(train_texts)}\")\n",
    "print(f\"Number of validation samples: {len(val_texts)}\")\n",
    "print(f\"Number of testing samples: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Construct a Multi-Layer Perceptron (MLP) model. (20%)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=10000, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=64, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "############### Parameters ############### \n",
      "Total Trainable Parameters: 5293122\n"
     ]
    }
   ],
   "source": [
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 2)  #output with 2 labels as speciied\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Example input size (to be updated based on feature extraction method)\n",
    "input_size = 10000  #based on the provided model architecture. \n",
    "\n",
    "# Initialize model\n",
    "mlp_model_ = MLPClassifier(input_size)\n",
    "print(mlp_model_)\n",
    "\n",
    "# Count trainable parameters\n",
    "print(\"\\n\\n############### Parameters ############### \")\n",
    "total_params = sum(p.numel() for p in mlp_model_.parameters() if p.requires_grad)\n",
    "print(f\"Total Trainable Parameters: {total_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Case 1: Implement Bag-of-Words (BoW)\n",
    "\n",
    "BoW is a text representation technique where a document is converted into a vector based on word frequency, ignoring word order and semantics. Each unique word in the vocabulary becomes a feature, and its value represents the number of times it appears in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Feature Shape: torch.Size([5536, 10000])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create Bag-of-Words vectorizer\n",
    "vectorizer = CountVectorizer(max_features=10000)\n",
    "#text to vector operator, limits vocab to keep the 10k most frequent words \n",
    "\n",
    "# Fit and transform the text data\n",
    "X_train_bow = vectorizer.fit_transform(train_texts).toarray()\n",
    "X_val_bow = vectorizer.transform(val_texts).toarray()\n",
    "#vocabulary is learnt \n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_bow = torch.tensor(X_train_bow, dtype=torch.float32)\n",
    "X_val_bow = torch.tensor(X_val_bow, dtype=torch.float32)\n",
    "y_train = torch.tensor(train_labels.values, dtype=torch.long)\n",
    "y_val = torch.tensor(val_labels.values, dtype=torch.long)\n",
    "#converting np array to torch tensor \n",
    "\n",
    "print(f\"BoW Feature Shape: {X_train_bow.shape}\")\n",
    "# (num_samples, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Case 2: Implement LLaMA-3.1 Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U \"huggingface_hub[cli]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: (768,)\n"
     ]
    }
   ],
   "source": [
    "# Define model name\n",
    "model_name = \"google-bert/bert-base-uncased\"\n",
    "\n",
    "# Load tokenizer and model on CPU\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.pad_token if tokenizer.pad_token else \"[PAD]\"\n",
    "bert_model = AutoModel.from_pretrained(model_name).to(\"cpu\")\n",
    "\n",
    "def get_bert_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(\"cpu\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "\n",
    "    # Mean pooling over the sequence dimension\n",
    "    return outputs.last_hidden_state.mean(dim=1).cpu().numpy().flatten()\n",
    "\n",
    "# # Function to get embeddings\n",
    "# def get_bert_embeddings(texts, batch_size=8):\n",
    "#     all_embeddings = []\n",
    "    \n",
    "#     for i in range(0, len(texts), batch_size):\n",
    "#         batch_texts = texts[i : i + batch_size]\n",
    "#         inputs = tokenizer(batch_texts, return_tensors=\"pt\", truncation=True, padding=True).to(\"cpu\")\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             outputs = bert_model(**inputs)\n",
    "        \n",
    "#         batch_embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "#         all_embeddings.append(batch_embeddings)\n",
    "\n",
    "#     return np.vstack(all_embeddings)\n",
    "\n",
    "# Example usage\n",
    "text = \"Implement case 2: Construct a function to use LLaMa-3.1 embeddings.\"\n",
    "embedding = get_bert_embeddings(text)\n",
    "print(\"Embedding shape:\", embedding.shape)  # (1, hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Train the model with 10 epochs and create the best-performing model (checkpoint.pt) on the Dataset 1. (10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DataLoaders\n",
    "batch_size = 32\n",
    "train_dataset = TensorDataset(X_train_bow, y_train)\n",
    "val_dataset = TensorDataset(X_val_bow, y_val)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory to save models\n",
    "model_dir = \"saved_models_bow\"\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 99.1350, Val Accuracy: 0.7912\n",
      "Best model saved: saved_models_bow/MLP_BoW_E1_Acc0.7912.pt\n",
      "Epoch 2/10, Loss: 35.9472, Val Accuracy: 0.7955\n",
      "Best model saved: saved_models_bow/MLP_BoW_E2_Acc0.7955.pt\n",
      "Epoch 3/10, Loss: 7.0539, Val Accuracy: 0.7847\n",
      "Epoch 4/10, Loss: 1.6393, Val Accuracy: 0.7970\n",
      "Best model saved: saved_models_bow/MLP_BoW_E4_Acc0.7970.pt\n",
      "Epoch 5/10, Loss: 0.3940, Val Accuracy: 0.7876\n",
      "Epoch 6/10, Loss: 0.2624, Val Accuracy: 0.7811\n",
      "Epoch 7/10, Loss: 0.4530, Val Accuracy: 0.7999\n",
      "Best model saved: saved_models_bow/MLP_BoW_E7_Acc0.7999.pt\n",
      "Epoch 8/10, Loss: 0.4430, Val Accuracy: 0.8006\n",
      "Best model saved: saved_models_bow/MLP_BoW_E8_Acc0.8006.pt\n",
      "Epoch 9/10, Loss: 0.3663, Val Accuracy: 0.7955\n",
      "Epoch 10/10, Loss: 0.0162, Val Accuracy: 0.7941\n",
      "\n",
      "Best Validation Accuracy: 0.8006\n"
     ]
    }
   ],
   "source": [
    "input_size = 10000\n",
    "mlp_model_ = MLPClassifier(input_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mlp_model_.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "best_acc = 0.0  # Track the best validation accuracy\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    mlp_model_.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mlp_model_(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Validation phase\n",
    "    mlp_model_.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            outputs = mlp_model_(X_batch)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    val_acc = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss:.4f}, Val Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "    # Save best model\n",
    "    # if val_acc > best_acc:\n",
    "    #     best_acc = val_acc\n",
    "    #     torch.save(model.state_dict(), f\"checkpoint_BoW_{best_acc}.pt\")\n",
    "\n",
    "    model_path = os.path.join(model_dir, f\"MLP_BoW_E{epoch+1}_Acc{val_acc:.4f}.pt\")\n",
    "\n",
    "    # Save best model\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save(mlp_model_.state_dict(), model_path)\n",
    "        print(f\"Best model saved: {model_path}\")\n",
    "\n",
    "print(f\"\\nBest Validation Accuracy: {best_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert-Base-Uncased Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir_two = \"saved_models_bert\"\n",
    "os.makedirs(model_dir_two, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Embeddings \n",
    "\n",
    "We are generating text embeddings using a pre-trained BERT model and preparing them for training a machine learning model. First, We load the tokenizer and model, ensuring that a valid padding token is set. Then, We define a function to convert input text into numerical embeddings by tokenizing the text, passing it through the model, and averaging the hidden states. Using `tqdm`, We apply this function to Wer training and validation text datasets while displaying a progress bar. Finally, We convert the generated embeddings and labels into PyTorch tensors and create `DataLoader` objects, which allow efficient batch processing during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Processing Train Embeddings: 100%|██████████| 5536/5536 [02:19<00:00, 39.58it/s]\n",
      "Processing Validation Embeddings: 100%|██████████| 1384/1384 [00:34<00:00, 40.08it/s]\n"
     ]
    }
   ],
   "source": [
    "# Convert training data to embeddings with progress bar\n",
    "train_embeddings = [get_bert_embeddings(text) for text in tqdm(train_texts, desc=\"Processing Train Embeddings\")]\n",
    "val_embeddings = [get_bert_embeddings(text) for text in tqdm(val_texts, desc=\"Processing Validation Embeddings\")]\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "train_embeddings = np.array(train_embeddings)  # Shape: (num_train_samples, 768)\n",
    "val_embeddings = np.array(val_embeddings)      # Shape: (num_val_samples, 768)\n",
    "\n",
    "# Convert labels to tensors\n",
    "y_train = torch.tensor(train_labels.values, dtype=torch.long)\n",
    "y_val = torch.tensor(val_labels.values, dtype=torch.long)\n",
    "\n",
    "# Convert embeddings to PyTorch tensors\n",
    "X_train = torch.tensor(train_embeddings, dtype=torch.float32)\n",
    "X_val = torch.tensor(val_embeddings, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder path\n",
    "save_dir = \"saved_embeddings_bert\"\n",
    "os.makedirs(save_dir, exist_ok=True)  # Create the folder if it doesn't exist\n",
    "\n",
    "# Save embeddings\n",
    "torch.save(X_train, os.path.join(save_dir, 'X_train.pt'))\n",
    "torch.save(X_val, os.path.join(save_dir, 'X_val.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5536, 768)\n"
     ]
    }
   ],
   "source": [
    "print(train_embeddings.shape)  # Should be (num_samples, feature_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Time: 0.19s, Loss: 73.3209, Train Acc: 0.7959\n",
      "Validation Accuracy: 0.8562\n",
      "Best model saved: saved_models_bert/MLP_BERT_E1_Acc0.8562.pt\n",
      "Epoch 2/10, Time: 0.18s, Loss: 59.3368, Train Acc: 0.8515\n",
      "Validation Accuracy: 0.8490\n",
      "Epoch 3/10, Time: 0.18s, Loss: 54.9672, Train Acc: 0.8629\n",
      "Validation Accuracy: 0.8490\n",
      "Epoch 4/10, Time: 0.19s, Loss: 50.8761, Train Acc: 0.8712\n",
      "Validation Accuracy: 0.8605\n",
      "Best model saved: saved_models_bert/MLP_BERT_E4_Acc0.8605.pt\n",
      "Epoch 5/10, Time: 0.17s, Loss: 46.9093, Train Acc: 0.8844\n",
      "Validation Accuracy: 0.8663\n",
      "Best model saved: saved_models_bert/MLP_BERT_E5_Acc0.8663.pt\n",
      "Epoch 6/10, Time: 0.17s, Loss: 42.8001, Train Acc: 0.8929\n",
      "Validation Accuracy: 0.8671\n",
      "Best model saved: saved_models_bert/MLP_BERT_E6_Acc0.8671.pt\n",
      "Epoch 7/10, Time: 0.18s, Loss: 38.4778, Train Acc: 0.9034\n",
      "Validation Accuracy: 0.8598\n",
      "Epoch 8/10, Time: 0.18s, Loss: 32.9764, Train Acc: 0.9207\n",
      "Validation Accuracy: 0.8468\n",
      "Epoch 9/10, Time: 0.18s, Loss: 30.1700, Train Acc: 0.9290\n",
      "Validation Accuracy: 0.8548\n",
      "Epoch 10/10, Time: 0.19s, Loss: 25.2177, Train Acc: 0.9454\n",
      "Validation Accuracy: 0.8569\n",
      "Best Validation Accuracy: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Move model to device\n",
    "device = \"cpu\"\n",
    "\n",
    "# Initialize model\n",
    "input_size = X_train.shape[-1]  # Ensures correct shape\n",
    "mlp_model_bert = MLPClassifier(input_size).to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mlp_model_bert.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "best_val_acc = 0.0\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    mlp_model_bert.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)  # Move data to device\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = mlp_model_bert(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        correct += (preds == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "\n",
    "    train_acc = correct / total\n",
    "    epoch_time = time.time() - start_time\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Time: {epoch_time:.2f}s, Loss: {total_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "\n",
    "    # Validation step\n",
    "    mlp_model_bert.eval()\n",
    "    correct, total = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)  # Move data to device\n",
    "            outputs = mlp_model_bert(X_batch)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct += (preds == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "\n",
    "    val_acc = correct / total\n",
    "    print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "    model_path = os.path.join(model_dir_two, f\"MLP_BERT_E{epoch+1}_Acc{val_acc:.4f}.pt\")\n",
    "\n",
    "    # Save best model\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save(mlp_model_.state_dict(), model_path)\n",
    "        print(f\"Best model saved: {model_path}\")\n",
    "\n",
    "\n",
    "print(f\"Best Validation Accuracy: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Have not touched code below this \n",
    "Save and Load Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Save final model checkpoint\u001b[39;00m\n\u001b[1;32m      2\u001b[0m final_checkpoint \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: optimizer\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhyperparameters\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_size\u001b[39m\u001b[38;5;124m'\u001b[39m: input_size,\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_sizes\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m64\u001b[39m],  \u001b[38;5;66;03m# MLP architecture\u001b[39;00m\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m,  \u001b[38;5;66;03m# Binary classification\u001b[39;00m\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.0001\u001b[39m,\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m: epochs\n\u001b[1;32m     11\u001b[0m     },\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining_history\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minitial_task\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSST-2\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransfer_task\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIMDB\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msst2_performance\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[1;32m     16\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: loss\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[1;32m     17\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: accuracy\n\u001b[1;32m     18\u001b[0m         },\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimdb_performance\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[1;32m     20\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: loss\u001b[38;5;241m.\u001b[39mitem(),  \u001b[38;5;66;03m# Update with IMDB results after training\u001b[39;00m\n\u001b[1;32m     21\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: accuracy  \u001b[38;5;66;03m# Update with IMDB results\u001b[39;00m\n\u001b[1;32m     22\u001b[0m         }\n\u001b[1;32m     23\u001b[0m     }\n\u001b[1;32m     24\u001b[0m }\n\u001b[1;32m     26\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(final_checkpoint, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmlp_text_classification_checkpoint.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining complete! Final model checkpoint saved.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Save final model checkpoint\n",
    "final_checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'hyperparameters': {\n",
    "        'input_size': input_size,\n",
    "        'hidden_sizes': [512, 256, 128, 64],  # MLP architecture\n",
    "        'output_size': 2,  # Binary classification\n",
    "        'learning_rate': 0.0001,\n",
    "        'epochs': epochs\n",
    "    },\n",
    "    'training_history': {\n",
    "        'initial_task': 'SST-2',\n",
    "        'transfer_task': 'IMDB',\n",
    "        'sst2_performance': {\n",
    "            'final_loss': loss.item(),\n",
    "            'final_accuracy': accuracy\n",
    "        },\n",
    "        'imdb_performance': {\n",
    "            'final_loss': loss.item(),  # Update with IMDB results after training\n",
    "            'final_accuracy': accuracy  # Update with IMDB results\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "torch.save(final_checkpoint, 'mlp_text_classification_checkpoint.pth')\n",
    "print(\"\\nTraining complete! Final model checkpoint saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Prepare the IMDB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the IMDB dataset (Note: this dataset contains a \"review\" column and a \"sentiment\" column)\n",
    "imdb_url = \"https://raw.githubusercontent.com/Ankit152/IMDB-sentiment-analysis/master/IMDB-Dataset.csv\"\n",
    "imdb_df = pd.read_csv(imdb_url)\n",
    "\n",
    "# Convert sentiment to numerical labels (assuming 'positive' and 'negative')\n",
    "imdb_df['label'] = imdb_df['sentiment'].apply(lambda x: 1 if x.lower() == 'positive' else 0)\n",
    "\n",
    "# Split into training and validation sets (80% training, 20% validation)\n",
    "imdb_train_df, imdb_val_df = train_test_split(imdb_df, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"IMDB training samples: {len(imdb_train_df)}\")\n",
    "print(f\"IMDB validation samples: {len(imdb_val_df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Feature Extraction and Tensor Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Extract the text and labels from the IMDB training and validation sets\n",
    "imdb_train_texts = imdb_train_df['review']\n",
    "imdb_train_labels = imdb_train_df['label']\n",
    "imdb_val_texts = imdb_val_df['review']\n",
    "imdb_val_labels = imdb_val_df['label']\n",
    "\n",
    "# Use the same CountVectorizer (Bag-of-Words) that was fit on the SST-2 dataset\n",
    "# (Assuming 'vectorizer' has been previously defined and fitted on the training texts of SST-2)\n",
    "X_train_imdb = vectorizer.transform(imdb_train_texts).toarray()\n",
    "X_val_imdb = vectorizer.transform(imdb_val_texts).toarray()\n",
    "\n",
    "# Convert features and labels to PyTorch tensors\n",
    "X_train_imdb = torch.tensor(X_train_imdb, dtype=torch.float32)\n",
    "X_val_imdb = torch.tensor(X_val_imdb, dtype=torch.float32)\n",
    "y_train_imdb = torch.tensor(imdb_train_labels.values, dtype=torch.long)\n",
    "y_val_imdb = torch.tensor(imdb_val_labels.values, dtype=torch.long)\n",
    "\n",
    "print(f\"IMDB BoW Training Features Shape: {X_train_imdb.shape}\")\n",
    "print(f\"IMDB BoW Validation Features Shape: {X_val_imdb.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continual Learning – Fine-tune on IMDB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define or re-use your model, criterion, and optimizer.\n",
    "# (Assuming 'model' is already defined and was trained on SST-2)\n",
    "# For continual learning, you might use a smaller learning rate.\n",
    "transfer_lr = 0.0001\n",
    "optimizer = optim.Adam(model.parameters(), lr=transfer_lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 10  # or as required\n",
    "\n",
    "# Fine-tune the model on the IMDB training set\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_imdb)\n",
    "    loss = criterion(outputs, y_train_imdb)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"IMDB Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Evaluate on the IMDB validation set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    val_outputs = model(X_val_imdb)\n",
    "    val_loss = criterion(val_outputs, y_val_imdb)\n",
    "    val_preds = val_outputs.argmax(dim=1)\n",
    "    accuracy = (val_preds == y_val_imdb).float().mean().item()\n",
    "\n",
    "print(f\"\\nIMDB Validation Loss: {val_loss.item():.4f}, Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    val_preds = model(X_val).argmax(dim=1)\n",
    "\n",
    "# Compute Accuracy\n",
    "accuracy = accuracy_score(y_val, val_preds)\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_val, val_preds)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization (TensorBoard Integration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(\"runs/text_classification\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    writer.add_scalar(\"Loss/train\", loss.item(), epoch)\n",
    "    writer.add_scalar(\"Loss/validation\", val_loss.item(), epoch)\n",
    "\n",
    "writer.close()\n",
    "print(\"TensorBoard logs saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
