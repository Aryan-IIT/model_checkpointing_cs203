{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aryan/Desktop/Academics /Semester 4/AI Software tools and techniques/model_checkpointing_cs203/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(2)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset Preparation (10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            sentence  label\n",
      "0  a stirring , funny and finally transporting re...      1\n",
      "1  apparently reassembled from the cutting room f...      0\n",
      "2  they presume their audience wo n't sit still f...      0\n",
      "3  this is a visually stunning rumination on love...      1\n",
      "4  jonathan parker 's bartleby should have been t...      1\n",
      "\n",
      "\n",
      "Number of training samples: 5536\n",
      "Number of validation samples: 1384\n",
      "Number of testing samples: 1821\n"
     ]
    }
   ],
   "source": [
    "# Define correct column names\n",
    "column_names = [\"sentence\", \"label\"]\n",
    "\n",
    "def load_dataset1(train_url, test_url):\n",
    "    train_df = pd.read_csv(train_url, sep='\\t', names=column_names, header=None)\n",
    "    test_df = pd.read_csv(test_url, sep='\\t', names=column_names, header=None)\n",
    "    return train_df, test_df\n",
    "\n",
    "# URLs for SST2 dataset\n",
    "train_url = \"https://raw.githubusercontent.com/clairett/pytorch-sentiment-classification/master/data/SST2/train.tsv\"\n",
    "test_url = \"https://raw.githubusercontent.com/clairett/pytorch-sentiment-classification/master/data/SST2/test.tsv\"\n",
    "\n",
    "# Load datasets\n",
    "train_df, test_df = load_dataset1(train_url, test_url)\n",
    "\n",
    "# Display first few rows to confirm correct loading\n",
    "print(train_df.head())\n",
    "\n",
    "\n",
    "# Use the 'sentence' column since SST2 uses it instead of 'text'\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_df['sentence'], train_df['label'], test_size=0.2, random_state=42\n",
    ")\n",
    "print(\"\\n\")\n",
    "print(f\"Number of training samples: {len(train_texts)}\")\n",
    "print(f\"Number of validation samples: {len(val_texts)}\")\n",
    "print(f\"Number of testing samples: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Construct a Multi-Layer Perceptron (MLP) model. (20%)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=10000, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=64, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "############### Parameters ############### \n",
      "Total Trainable Parameters: 5293122\n"
     ]
    }
   ],
   "source": [
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 2)  #output with 2 labels as speciied\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Example input size (to be updated based on feature extraction method)\n",
    "input_size = 10000  #based on the provided model architecture. \n",
    "\n",
    "# Initialize model\n",
    "mlp_model_ = MLPClassifier(input_size)\n",
    "print(mlp_model_)\n",
    "\n",
    "# Count trainable parameters\n",
    "print(\"\\n\\n############### Parameters ############### \")\n",
    "total_params = sum(p.numel() for p in mlp_model_.parameters() if p.requires_grad)\n",
    "print(f\"Total Trainable Parameters: {total_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Case 1: Implement Bag-of-Words (BoW)\n",
    "\n",
    "BoW is a text representation technique where a document is converted into a vector based on word frequency, ignoring word order and semantics. Each unique word in the vocabulary becomes a feature, and its value represents the number of times it appears in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Feature Shape: torch.Size([5536, 10000])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create Bag-of-Words vectorizer\n",
    "vectorizer = CountVectorizer(max_features=10000)\n",
    "#text to vector operator, limits vocab to keep the 10k most frequent words \n",
    "\n",
    "# Fit and transform the text data\n",
    "X_train_bow = vectorizer.fit_transform(train_texts).toarray()\n",
    "X_val_bow = vectorizer.transform(val_texts).toarray()\n",
    "#vocabulary is learnt \n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_bow = torch.tensor(X_train_bow, dtype=torch.float32)\n",
    "X_val_bow = torch.tensor(X_val_bow, dtype=torch.float32)\n",
    "y_train = torch.tensor(train_labels.values, dtype=torch.long)\n",
    "y_val = torch.tensor(val_labels.values, dtype=torch.long)\n",
    "#converting np array to torch tensor \n",
    "\n",
    "print(f\"BoW Feature Shape: {X_train_bow.shape}\")\n",
    "# (num_samples, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Case 2: Implement LLaMA-3.1 Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U \"huggingface_hub[cli]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: (768,)\n"
     ]
    }
   ],
   "source": [
    "# Define model name\n",
    "model_name = \"google-bert/bert-base-uncased\"\n",
    "\n",
    "# Load tokenizer and model on CPU\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.pad_token if tokenizer.pad_token else \"[PAD]\"\n",
    "bert_model = AutoModel.from_pretrained(model_name).to(\"cpu\")\n",
    "\n",
    "def get_bert_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(\"cpu\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "\n",
    "    # Mean pooling over the sequence dimension\n",
    "    return outputs.last_hidden_state.mean(dim=1).cpu().numpy().flatten()\n",
    "\n",
    "# # Function to get embeddings\n",
    "# def get_bert_embeddings(texts, batch_size=8):\n",
    "#     all_embeddings = []\n",
    "    \n",
    "#     for i in range(0, len(texts), batch_size):\n",
    "#         batch_texts = texts[i : i + batch_size]\n",
    "#         inputs = tokenizer(batch_texts, return_tensors=\"pt\", truncation=True, padding=True).to(\"cpu\")\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             outputs = bert_model(**inputs)\n",
    "        \n",
    "#         batch_embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "#         all_embeddings.append(batch_embeddings)\n",
    "\n",
    "#     return np.vstack(all_embeddings)\n",
    "\n",
    "# Example usage\n",
    "text = \"Implement case 2: Construct a function to use LLaMa-3.1 embeddings.\"\n",
    "embedding = get_bert_embeddings(text)\n",
    "print(\"Embedding shape:\", embedding.shape)  # (1, hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Train the model with 10 epochs and create the best-performing model (checkpoint.pt) on the Dataset 1. (10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DataLoaders\n",
    "batch_size = 32\n",
    "train_dataset = TensorDataset(X_train_bow, y_train)\n",
    "val_dataset = TensorDataset(X_val_bow, y_val)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory to save models\n",
    "model_dir = \"saved_models_bow\"\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 98.7630, Val Accuracy: 0.8013\n",
      "Best model saved: saved_models_bow\\MLP_BoW_E1_Acc0.8013.pt\n",
      "Epoch 2/10, Loss: 36.6062, Val Accuracy: 0.8064\n",
      "Best model saved: saved_models_bow\\MLP_BoW_E2_Acc0.8064.pt\n",
      "Epoch 3/10, Loss: 7.1429, Val Accuracy: 0.7897\n",
      "Epoch 4/10, Loss: 1.2432, Val Accuracy: 0.7941\n",
      "Epoch 5/10, Loss: 0.2780, Val Accuracy: 0.7897\n",
      "Epoch 6/10, Loss: 0.2761, Val Accuracy: 0.7948\n",
      "Epoch 7/10, Loss: 0.0547, Val Accuracy: 0.7984\n",
      "Epoch 8/10, Loss: 0.0069, Val Accuracy: 0.7905\n",
      "Epoch 9/10, Loss: 0.0014, Val Accuracy: 0.7948\n",
      "Epoch 10/10, Loss: 0.0006, Val Accuracy: 0.7948\n",
      "\n",
      "Best Validation Accuracy: 0.8064\n"
     ]
    }
   ],
   "source": [
    "input_size = 10000\n",
    "mlp_model_ = MLPClassifier(input_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mlp_model_.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "best_acc = 0.0  # Track the best validation accuracy\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    mlp_model_.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mlp_model_(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Validation phase\n",
    "    mlp_model_.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            outputs = mlp_model_(X_batch)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    val_acc = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss:.4f}, Val Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "    # Save best model\n",
    "    # if val_acc > best_acc:\n",
    "    #     best_acc = val_acc\n",
    "    #     torch.save(model.state_dict(), f\"checkpoint_BoW_{best_acc}.pt\")\n",
    "\n",
    "    model_path = os.path.join(model_dir, f\"MLP_BoW_E{epoch+1}_Acc{val_acc:.4f}.pt\")\n",
    "\n",
    "    # Save best model\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save(mlp_model_.state_dict(), model_path)\n",
    "        print(f\"Best model saved: {model_path}\")\n",
    "\n",
    "print(f\"\\nBest Validation Accuracy: {best_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert-Base-Uncased Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir_two = \"saved_models_bert\"\n",
    "os.makedirs(model_dir_two, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Embeddings \n",
    "\n",
    "We are generating text embeddings using a pre-trained BERT model and preparing them for training a machine learning model. First, We load the tokenizer and model, ensuring that a valid padding token is set. Then, We define a function to convert input text into numerical embeddings by tokenizing the text, passing it through the model, and averaging the hidden states. Using `tqdm`, We apply this function to Wer training and validation text datasets while displaying a progress bar. Finally, We convert the generated embeddings and labels into PyTorch tensors and create `DataLoader` objects, which allow efficient batch processing during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Train Embeddings: 100%|██████████| 5536/5536 [02:02<00:00, 45.25it/s]\n",
      "Processing Validation Embeddings: 100%|██████████| 1384/1384 [00:30<00:00, 45.70it/s]\n"
     ]
    }
   ],
   "source": [
    "# Convert training data to embeddings with progress bar\n",
    "train_embeddings = [get_bert_embeddings(text) for text in tqdm(train_texts, desc=\"Processing Train Embeddings\")]\n",
    "val_embeddings = [get_bert_embeddings(text) for text in tqdm(val_texts, desc=\"Processing Validation Embeddings\")]\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "train_embeddings = np.array(train_embeddings)  # Shape: (num_train_samples, 768)\n",
    "val_embeddings = np.array(val_embeddings)      # Shape: (num_val_samples, 768)\n",
    "\n",
    "# Convert labels to tensors\n",
    "y_train = torch.tensor(train_labels.values, dtype=torch.long)\n",
    "y_val = torch.tensor(val_labels.values, dtype=torch.long)\n",
    "\n",
    "# Convert embeddings to PyTorch tensors\n",
    "X_train = torch.tensor(train_embeddings, dtype=torch.float32)\n",
    "X_val = torch.tensor(val_embeddings, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder path\n",
    "save_dir = \"saved_embeddings_bert_dataset1\"\n",
    "os.makedirs(save_dir, exist_ok=True)  # Create the folder if it doesn't exist\n",
    "\n",
    "# Save embeddings\n",
    "torch.save(X_train, os.path.join(save_dir, 'X_train.pt'))\n",
    "torch.save(X_val, os.path.join(save_dir, 'X_val.pt'))\n",
    "\n",
    "torch.save(y_train, os.path.join(save_dir, 'y_train.pt'))\n",
    "torch.save(y_val, os.path.join(save_dir, 'y_val.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5536, 768)\n"
     ]
    }
   ],
   "source": [
    "print(train_embeddings.shape)  # Should be (num_samples, feature_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_size = X_train.shape[-1]\n",
    "# input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Time: 0.21s, Loss: 76.5805, Train Acc: 0.7820\n",
      "Validation Accuracy: 0.8389\n",
      "Epoch 2/10, Time: 0.18s, Loss: 57.7846, Train Acc: 0.8510\n",
      "Validation Accuracy: 0.8613\n",
      "Epoch 3/10, Time: 0.18s, Loss: 55.1085, Train Acc: 0.8618\n",
      "Validation Accuracy: 0.8613\n",
      "Epoch 4/10, Time: 0.18s, Loss: 50.7318, Train Acc: 0.8728\n",
      "Validation Accuracy: 0.8591\n",
      "Epoch 5/10, Time: 0.18s, Loss: 46.5990, Train Acc: 0.8864\n",
      "Validation Accuracy: 0.8490\n",
      "Epoch 6/10, Time: 0.18s, Loss: 44.8490, Train Acc: 0.8887\n",
      "Validation Accuracy: 0.8707\n",
      "Epoch 7/10, Time: 0.21s, Loss: 38.3971, Train Acc: 0.9062\n",
      "Validation Accuracy: 0.8468\n",
      "Epoch 8/10, Time: 0.18s, Loss: 34.3140, Train Acc: 0.9165\n",
      "Validation Accuracy: 0.8663\n",
      "Epoch 9/10, Time: 0.19s, Loss: 30.0379, Train Acc: 0.9254\n",
      "Validation Accuracy: 0.8562\n",
      "Epoch 10/10, Time: 0.18s, Loss: 25.7198, Train Acc: 0.9388\n",
      "Validation Accuracy: 0.8649\n"
     ]
    }
   ],
   "source": [
    "# Move model to device\n",
    "device = \"cpu\"\n",
    "\n",
    "# Initialize model\n",
    "input_size = X_train.shape[-1]  # Ensures correct shape\n",
    "mlp_model_bert = MLPClassifier(input_size).to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mlp_model_bert.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "best_acc = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    mlp_model_bert.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)  # Move data to device\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = mlp_model_bert(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        correct += (preds == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "\n",
    "    train_acc = correct / total\n",
    "    epoch_time = time.time() - start_time\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Time: {epoch_time:.2f}s, Loss: {total_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "\n",
    "    # Validation step\n",
    "    mlp_model_bert.eval()\n",
    "    correct, total = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)  # Move data to device\n",
    "            outputs = mlp_model_bert(X_batch)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct += (preds == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "\n",
    "    val_acc = correct / total\n",
    "    print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "    model_path = os.path.join(model_dir_two, f\"MLP_BERT_E{epoch+1}_Acc{val_acc:.4f}.pt\")\n",
    "\n",
    "    # Save best model\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save(mlp_model_bert.state_dict(), model_path)\n",
    "        # print(f\"Best model saved: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Linear(in_features=64, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_model_bert #for reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Use the checkpoint from before and train on the IMDB dataset (Dataset 2). (10%)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Preprocess IMDB Dataset (Bag of Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB BoW Training Features Shape: torch.Size([40000, 10000])\n",
      "IMDB BoW Validation Features Shape: torch.Size([10000, 10000])\n"
     ]
    }
   ],
   "source": [
    "# Load the IMDB dataset\n",
    "imdb_url = \"https://raw.githubusercontent.com/Ankit152/IMDB-sentiment-analysis/master/IMDB-Dataset.csv\"\n",
    "imdb_df = pd.read_csv(imdb_url)\n",
    "\n",
    "# Convert sentiment to numerical labels ('positive' -> 1, 'negative' -> 0)\n",
    "imdb_df['label'] = imdb_df['sentiment'].apply(lambda x: 1 if x.lower() == 'positive' else 0)\n",
    "\n",
    "# Split dataset (80% training, 20% validation)\n",
    "imdb_train_df, imdb_val_df = train_test_split(imdb_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Extract text and labels\n",
    "imdb_train_texts = imdb_train_df['review']\n",
    "imdb_train_labels = imdb_train_df['label']\n",
    "imdb_val_texts = imdb_val_df['review']\n",
    "imdb_val_labels = imdb_val_df['label']\n",
    "\n",
    "# Initialize CountVectorizer for Bag-of-Words representation\n",
    "vectorizer = CountVectorizer(max_features=10000)  # Limit vocabulary size to 10,000\n",
    "X_train_imdb = vectorizer.fit_transform(imdb_train_texts).toarray()\n",
    "X_val_imdb = vectorizer.transform(imdb_val_texts).toarray()\n",
    "\n",
    "# Convert features and labels to PyTorch tensors\n",
    "X_train_imdb = torch.tensor(X_train_imdb, dtype=torch.float32)\n",
    "X_val_imdb = torch.tensor(X_val_imdb, dtype=torch.float32)\n",
    "y_train_imdb = torch.tensor(imdb_train_labels.values, dtype=torch.long)\n",
    "y_val_imdb = torch.tensor(imdb_val_labels.values, dtype=torch.long)\n",
    "\n",
    "print(f\"IMDB BoW Training Features Shape: {X_train_imdb.shape}\")\n",
    "print(f\"IMDB BoW Validation Features Shape: {X_val_imdb.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Define the MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully loaded!\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the model\n",
    "input_size = 10000  # This should match the feature extraction method (BoW feature size)\n",
    "model_IMDB_bow = MLPClassifier(input_size).to(device)\n",
    "\n",
    "# Load the saved model weights\n",
    "checkpoint_path = \"saved_models_bow/MLP_BoW_E8_Acc0.8006.pt\"\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)  # Load on correct device\n",
    "model_IMDB_bow.load_state_dict(checkpoint)  # Load weights\n",
    "\n",
    "# Set model to evaluation mode for inference\n",
    "model_IMDB_bow.eval()\n",
    "\n",
    "print(\"Model successfully loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 2.9406\n",
      "Epoch 2/10, Loss: 0.2437\n",
      "Epoch 3/10, Loss: 0.1811\n",
      "Epoch 4/10, Loss: 0.1425\n",
      "Epoch 5/10, Loss: 0.1247\n",
      "Epoch 6/10, Loss: 0.1340\n",
      "Epoch 7/10, Loss: 0.0935\n",
      "Epoch 8/10, Loss: 0.0504\n",
      "Epoch 9/10, Loss: 0.0417\n",
      "Epoch 10/10, Loss: 0.0462\n"
     ]
    }
   ],
   "source": [
    "epochs = 10  # Set number of epochs\n",
    "batch_size = 256  # Mini-batch size for training\n",
    "num_samples = X_train_imdb.shape[0]\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_IMDB_bow.parameters(), lr=0.0001)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model_IMDB_bow.train()  # Set to training mode\n",
    "    total_loss = 0\n",
    "\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        batch_X = X_train_imdb[i:i+batch_size]\n",
    "        batch_y = y_train_imdb[i:i+batch_size]\n",
    "\n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "        outputs = model_IMDB_bow(batch_X)  # Forward pass\n",
    "        loss = criterion(outputs, batch_y)  # Compute loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / (num_samples // batch_size)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Validation Loss and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.5707\n",
      "Validation Accuracy: 87.98%\n"
     ]
    }
   ],
   "source": [
    "model_IMDB_bow.eval()  # Set to evaluation mode\n",
    "total_val_loss = 0\n",
    "correct_predictions = 0\n",
    "num_val_samples = X_val_imdb.shape[0]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, num_val_samples, batch_size):\n",
    "        batch_X_val = X_val_imdb[i:i+batch_size]\n",
    "        batch_y_val = y_val_imdb[i:i+batch_size]\n",
    "\n",
    "        val_outputs = model_IMDB_bow(batch_X_val)  # Forward pass\n",
    "        val_loss = criterion(val_outputs, batch_y_val)  # Compute loss\n",
    "        total_val_loss += val_loss.item()\n",
    "\n",
    "        val_predictions = torch.argmax(val_outputs, dim=1)\n",
    "        correct_predictions += (val_predictions == batch_y_val).sum().item()\n",
    "\n",
    "# Compute Average Validation Loss\n",
    "avg_val_loss = total_val_loss / (num_val_samples // batch_size)\n",
    "\n",
    "# Compute Validation Accuracy\n",
    "val_accuracy = correct_predictions / num_val_samples\n",
    "\n",
    "print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Embedding for IMDB "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert IMDB Text to BERT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sample_size = 200\n",
    "# # imdb_train_sample = imdb_train_texts\n",
    "# # imdb_val_sample = imdb_val_texts\n",
    "\n",
    "# X_train_imdb_bert = get_bert_embeddings(imdb_train_texts)\n",
    "# X_val_imdb_bert = get_bert_embeddings(imdb_val_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Train Embeddings:   0%|          | 0/40000 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Processing Train Embeddings: 100%|██████████| 40000/40000 [1:03:04<00:00, 10.57it/s]  \n",
      "Processing Validation Embeddings: 100%|██████████| 10000/10000 [08:50<00:00, 18.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB BERT Training Features Shape: torch.Size([40000, 768])\n",
      "IMDB BERT Validation Features Shape: torch.Size([10000, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert IMDB training and validation texts into embeddings with progress bar\n",
    "X_train_imdb_bert = np.array([get_bert_embeddings(text) for text in tqdm(imdb_train_texts, desc=\"Processing Train Embeddings\")])\n",
    "X_val_imdb_bert = np.array([get_bert_embeddings(text) for text in tqdm(imdb_val_texts, desc=\"Processing Validation Embeddings\")])\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_imdb_bert = torch.tensor(X_train_imdb_bert, dtype=torch.float32)\n",
    "X_val_imdb_bert = torch.tensor(X_val_imdb_bert, dtype=torch.float32)\n",
    "y_train_imdb = torch.tensor(imdb_train_labels.values, dtype=torch.long)\n",
    "y_val_imdb = torch.tensor(imdb_val_labels.values, dtype=torch.long)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TensorDataset(X_train_imdb_bert, y_train_imdb)\n",
    "val_dataset = TensorDataset(X_val_imdb_bert, y_val_imdb)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"IMDB BERT Training Features Shape: {X_train_imdb_bert.shape}\")\n",
    "print(f\"IMDB BERT Validation Features Shape: {X_val_imdb_bert.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory to save models\n",
    "embd_dir = \"saved_bert_embed_imdb\"\n",
    "os.makedirs(embd_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT embeddings and labels saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save BERT embeddings for training and validation sets\n",
    "# os.path.join(save_dir, 'X_train.pt')\n",
    "np.save(os.path.join(embd_dir,\"IMDB_train_BERT_embeddings.npy\"), X_train_imdb_bert.numpy())\n",
    "np.save(os.path.join(embd_dir,\"IMDB_val_BERT_embeddings.npy\"), X_val_imdb_bert.numpy())\n",
    "\n",
    "# Save labels as well\n",
    "np.save(os.path.join(embd_dir,\"IMDB_train_labels.npy\"), y_train_imdb.numpy())\n",
    "np.save(os.path.join(embd_dir,\"IMDB_val_labels.npy\"), y_val_imdb.numpy())\n",
    "\n",
    "print(\"BERT embeddings and labels saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40000, 768])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_imdb_bert.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully loaded!\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the model\n",
    "input_size = X_train_imdb_bert.shape[-1]  # This should match the feature extraction method (BoW feature size)\n",
    "model_IMDB_bow = MLPClassifier(input_size).to(device)\n",
    "\n",
    "# Load the saved model weights\n",
    "checkpoint_path = \"saved_models_bert/MLP_BERT_E6_Acc0.8707.pt\"\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)  # Load on correct device\n",
    "model_IMDB_bow.load_state_dict(checkpoint)  # Load weights\n",
    "\n",
    "# Set model to evaluation mode for inference\n",
    "model_IMDB_bow.eval()\n",
    "\n",
    "print(\"Model successfully loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Train the MLP Model on BERT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Training Loss: 0.3413\n",
      "Epoch 2/10, Training Loss: 0.2856\n",
      "Epoch 3/10, Training Loss: 0.2772\n",
      "Epoch 4/10, Training Loss: 0.2708\n",
      "Epoch 5/10, Training Loss: 0.2648\n",
      "Epoch 6/10, Training Loss: 0.2588\n",
      "Epoch 7/10, Training Loss: 0.2527\n",
      "Epoch 8/10, Training Loss: 0.2462\n",
      "Epoch 9/10, Training Loss: 0.2396\n",
      "Epoch 10/10, Training Loss: 0.2327\n"
     ]
    }
   ],
   "source": [
    "# Define model for BERT embeddings\n",
    "input_size_bert = X_train_imdb_bert.shape[1]  # Adjust input size based on BERT embeddings\n",
    "model_IMDB_bert = MLPClassifier(input_size_bert)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_IMDB_bert.parameters(), lr=0.0001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "for epoch in range(epochs):\n",
    "    model_IMDB_bert.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for i in range(0, X_train_imdb_bert.shape[0], batch_size):\n",
    "        batch_X = X_train_imdb_bert[i:i+batch_size]\n",
    "        batch_y = y_train_imdb[i:i+batch_size]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_IMDB_bert(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / (X_train_imdb_bert.shape[0] // batch_size)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {avg_train_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Compute Validation Loss and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss (BERT): 0.2532\n",
      "Validation Accuracy (BERT): 89.53%\n"
     ]
    }
   ],
   "source": [
    "# Compute validation loss and accuracy\n",
    "model_IMDB_bert.eval()\n",
    "total_val_loss = 0\n",
    "correct_predictions = 0\n",
    "num_val_samples = X_val_imdb_bert.shape[0]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, num_val_samples, batch_size):\n",
    "        batch_X_val = X_val_imdb_bert[i:i+batch_size]\n",
    "        batch_y_val = y_val_imdb[i:i+batch_size]\n",
    "\n",
    "        val_outputs = model_IMDB_bert(batch_X_val)\n",
    "        val_loss = criterion(val_outputs, batch_y_val)\n",
    "        total_val_loss += val_loss.item()\n",
    "\n",
    "        val_predictions = torch.argmax(val_outputs, dim=1)\n",
    "        correct_predictions += (val_predictions == batch_y_val).sum().item()\n",
    "\n",
    "# Compute Average Validation Loss\n",
    "avg_val_loss = total_val_loss / (num_val_samples // batch_size)\n",
    "\n",
    "# Compute Validation Accuracy\n",
    "val_accuracy = correct_predictions / num_val_samples\n",
    "\n",
    "print(f\"Validation Loss (BERT): {avg_val_loss:.4f}\")\n",
    "print(f\"Validation Accuracy (BERT): {val_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization (TensorBoard Integration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(\"runs/text_classification\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    writer.add_scalar(\"Loss/train\", loss.item(), epoch)\n",
    "    writer.add_scalar(\"Loss/validation\", val_loss.item(), epoch)\n",
    "\n",
    "writer.close()\n",
    "print(\"TensorBoard logs saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
